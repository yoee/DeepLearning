# 논문 요약

<br>

PRNet은 하나의 RGB 얼굴 사진으로도 실시간 3-dimensional face reconstruction 뿐만 아니라,  pose alignment 와 dense feature tracking 까지 지원한다. 또한 노이즈인 illuminasion이나 얼굴 예측을 힘들게 했던 self-occlusion에 강하다.

<br>

##  Mesh and Position Map

3차원의 mesh 값을 convolution network에 넣기는 쉽지 않다. 53,215*3개의 예측이 x, y, z에 있기 때문이다 (왜 53215인지 모르겠음. face mesh는 그마마한 점 개수를 가진다고 함). Position map은 mesh의 (x, y, z) 값을 (r, g, b) 값에 대응 시킨 **position map**을 만들어 [256, 256] resolution의 이미지 만으로도 [256, 256, 3]의 배열을 예측해낼 수 있게 했다. 즉, 이제 네트워크에 들어갈 수 있게 된 것!

<br>

## The 300W-LP Dataset

하나의 2D 이미지에서 합성된 다양한 각도의 사진을 확인해 볼 수 있다. 이는 3DMM coefficient의 지표를 활용해서 가능했던 것인데, 3D point cloud는 이를 통해서 쉽게 생성되어질 수 있다.

300W-LP의 fitted mesh는 해당하는 이미지의 얼굴을 모델링 할 뿐만 아니라, 얼굴의 포즈 또한 align할 수있다. 

<br>

## Network

encode-decode 구조를 사용함. 얼굴의 핵심 부분인 central(눈, 코, 입)등에 강세를 둔 weighted mask를 둔 loss 함수와 노이즈를 통하여 occlusion을 상당 부분 해결할 수 있었다.

<br>

--------------

## 3.3 Training Details

#### (1문단 : training 과정과 data의 변화)

- 300W-LP를 data set으로 채용했다!
  - 여기서 LP는 large pose의 약자로, 사진을 보면 하나의 2D 이미지에서 합성된 다양한 각도의 사진을 확인해볼 수 있다.
  - 다양한 이미지가 합성될 수 있었던 것은 3DMM coefficient의 지표를 활용해서 가능했던 것인데
    - 3D point cloud는 3DMM coefficient를 통하여 쉽게 생성되어질 수 있다.
- 특별히, ground truth bounding box를 통해서 256*256으로 사진을 잘랐다.
  - ground truth (추론에 의한 것이 아닌, 직접적인 관찰에 의하여 얻은 정보)
- 그리고 잘린 사진의 annotated 3DMM parameters를 활용해서 3D position을 구해냈다.
- 이후, ground truth position map을 얻기 위하여 uv map으로 이를 렌더링 했다.
  - 여기서의 map size 또한 256*256 
  - 이 말은, 우리가 regress해야하는 45K이상의 point cloud가 있다는 말이다. (❓256*256 = 65,536이라서 그런걸까)
- 비록 우리의 trainind data는 3DMM으로부터 만들어졌지만, 우리의 네트워크의 결과물인 position map은 그 어떤 face template이나 3DMM의 linear space에 구애받지 않는다! (❓어떻게)

#### (2문단 : data set 전처리와 training parameters)

- 학습 data 처리
  - 단순한 처리들
    - randomly rotating (-45도~45도)
    - input size 변동 (10%)
    - scale 조정 (0.9~1.2)
  - Augumentation
    - scaling color
    - noise (occulsion 해결을 위하여)
- training parameters
  - Adam optimizer
    - learning rate 0.0001
    - 5 epochs 이후에 half decay
    - batch size 16

